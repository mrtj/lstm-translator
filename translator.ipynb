{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ae927a",
   "metadata": {},
   "source": [
    "# Word-level translator using LSTMs\n",
    "\n",
    "This notebook shows how to train with Amazon SageMaker an NLP model that translates from English to a series of languages. The list of languages is limited by the available training data.\n",
    "\n",
    "The model uses an LSTM based encoder / decoder architeecture, inspired by https://github.com/hlamba28/Word-Level-Eng-Mar-NMT.\n",
    "\n",
    "The training time depends on the dataset available. Typically an epoch is trained in 30 minutes for small datasets (for example, Hungarian) and 1.5 - 2 hours for big datasets (for example, Italian). With 15-20 epochs you can already get an acceptable result.\n",
    "\n",
    "The dataset was created from a curated list of anki flash cards (https://ankiweb.net/shared/decks/). Each translated sentences is annotated with CC license string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "sys.path.append('./src')\n",
    "\n",
    "from utils import preproc, load_vocab, save_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c9163",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e28cbf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_name</th>\n",
       "      <th>source_code</th>\n",
       "      <th>target_name</th>\n",
       "      <th>target_code</th>\n",
       "      <th>file_name</th>\n",
       "      <th>num_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>afr</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>afr-eng.zip</td>\n",
       "      <td>904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albanian</td>\n",
       "      <td>sqi</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>sqi-eng.zip</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algerian Arabic</td>\n",
       "      <td>arq</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>arq-eng.zip</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>ara</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>ara-eng.zip</td>\n",
       "      <td>12156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Assamese</td>\n",
       "      <td>asm</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>asm-eng.zip</td>\n",
       "      <td>1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Ukrainian</td>\n",
       "      <td>ukr</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>ukr-eng.zip</td>\n",
       "      <td>155019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Urdu</td>\n",
       "      <td>urd</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>urd-eng.zip</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Uyghur</td>\n",
       "      <td>uig</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>uig-eng.zip</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>vie</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>vie-eng.zip</td>\n",
       "      <td>8050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Waray</td>\n",
       "      <td>war</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>war-eng.zip</td>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source_name source_code target_name target_code    file_name  \\\n",
       "0         Afrikaans         afr     English         eng  afr-eng.zip   \n",
       "1          Albanian         sqi     English         eng  sqi-eng.zip   \n",
       "2   Algerian Arabic         arq     English         eng  arq-eng.zip   \n",
       "3            Arabic         ara     English         eng  ara-eng.zip   \n",
       "4          Assamese         asm     English         eng  asm-eng.zip   \n",
       "..              ...         ...         ...         ...          ...   \n",
       "77        Ukrainian         ukr     English         eng  ukr-eng.zip   \n",
       "78             Urdu         urd     English         eng  urd-eng.zip   \n",
       "79           Uyghur         uig     English         eng  uig-eng.zip   \n",
       "80       Vietnamese         vie     English         eng  vie-eng.zip   \n",
       "81            Waray         war     English         eng  war-eng.zip   \n",
       "\n",
       "   num_sentences  \n",
       "0            904  \n",
       "1            448  \n",
       "2            156  \n",
       "3          12156  \n",
       "4           1963  \n",
       "..           ...  \n",
       "77        155019  \n",
       "78          1146  \n",
       "79           281  \n",
       "80          8050  \n",
       "81          1147  \n",
       "\n",
       "[82 rows x 6 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_fields(l):\n",
    "    l = l.strip()\n",
    "    elems = l.split(' - ')\n",
    "    source_name = elems[0]\n",
    "    elems = elems[1].split()\n",
    "    target_name = elems[0]\n",
    "    file_name = elems[1]\n",
    "    num_sentences = elems[2].strip('()')\n",
    "    base_name = file_name.split('.')[0]\n",
    "    source_code, target_code = base_name.split('-')\n",
    "    return source_name, source_code, target_name, target_code, file_name, num_sentences\n",
    "    \n",
    "with open('./dataset/language_list.txt') as f:\n",
    "    data = [extract_fields(l) for l in f]\n",
    "    cols = ['source_name', 'source_code', 'target_name', 'target_code', 'file_name', 'num_sentences']\n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de365e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all possible language pairs\n",
    "\n",
    "dataset_path = Path('dataset')\n",
    "\n",
    "!mkdir -p {dataset_path}\n",
    "\n",
    "ip = get_ipython()\n",
    "for lang_code in df['source_code']:\n",
    "    ip.system(\n",
    "    f'''cd {dataset_path} && \\\n",
    "        curl -O http://www.manythings.org/anki/{lang_code}-eng.zip && \\\n",
    "        cd -\n",
    "    ''')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1a8e3",
   "metadata": {},
   "source": [
    "## Select target language code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0c23ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_code = 'ita'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ce689",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d1910acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354238, 2)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(dataset_path / f'{lang_code}-eng.zip') as archive:\n",
    "    with archive.open(f'{lang_code}.txt') as f:\n",
    "        lines = pd.read_table(f, names=['eng', lang_code, 'attrib'])\n",
    "lines = lines[['eng', lang_code]]\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "794f6e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>ita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>322525</th>\n",
       "      <td>tom asked me how to get to marys house</td>\n",
       "      <td>START_ tom mi ha chiesto come arrivare a casa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88966</th>\n",
       "      <td>i bought tom a clock</td>\n",
       "      <td>START_ ho comprato un orologio per tom _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43067</th>\n",
       "      <td>we told everyone</td>\n",
       "      <td>START_ noi labbiamo detto a tutti _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281477</th>\n",
       "      <td>maybe its best not to add pepper</td>\n",
       "      <td>START_ forse è meglio non aggiungere del pepe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254369</th>\n",
       "      <td>are you good at keeping secrets</td>\n",
       "      <td>START_ tu sei bravo a mantenere i segreti _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223327</th>\n",
       "      <td>tom seems to be really happy</td>\n",
       "      <td>START_ tom sembra essere veramente felice _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79304</th>\n",
       "      <td>she knows who he is</td>\n",
       "      <td>START_ sa chi è lui _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>we lost</td>\n",
       "      <td>START_ perdemmo _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292395</th>\n",
       "      <td>tom left boston in october of</td>\n",
       "      <td>START_ tom lasciò boston nellottobre del _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264503</th>\n",
       "      <td>whats the purpose of your trip</td>\n",
       "      <td>START_ qual è lo scopo del vostro viaggio _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           eng  \\\n",
       "322525  tom asked me how to get to marys house   \n",
       "88966                     i bought tom a clock   \n",
       "43067                         we told everyone   \n",
       "281477        maybe its best not to add pepper   \n",
       "254369         are you good at keeping secrets   \n",
       "223327            tom seems to be really happy   \n",
       "79304                      she knows who he is   \n",
       "550                                    we lost   \n",
       "292395           tom left boston in october of   \n",
       "264503          whats the purpose of your trip   \n",
       "\n",
       "                                                      ita  \n",
       "322525  START_ tom mi ha chiesto come arrivare a casa ...  \n",
       "88966         START_ ho comprato un orologio per tom _END  \n",
       "43067              START_ noi labbiamo detto a tutti _END  \n",
       "281477  START_ forse è meglio non aggiungere del pepe ...  \n",
       "254369     START_ tu sei bravo a mantenere i segreti _END  \n",
       "223327     START_ tom sembra essere veramente felice _END  \n",
       "79304                            START_ sa chi è lui _END  \n",
       "550                                  START_ perdemmo _END  \n",
       "292395      START_ tom lasciò boston nellottobre del _END  \n",
       "264503     START_ qual è lo scopo del vostro viaggio _END  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines['eng'] = lines['eng'].apply(preproc)\n",
    "lines[lang_code] = lines[lang_code].apply(preproc)\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "lines[lang_code] = lines[lang_code].apply(lambda x : 'START_ '+ x + ' _END')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9116722",
   "metadata": {},
   "source": [
    "## Create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c31ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(sentences):\n",
    "    vocab = set()\n",
    "    for text in sentences:\n",
    "        vocab.update(text.split())\n",
    "    return vocab\n",
    "\n",
    "all_source_words = get_vocab(lines['eng'])\n",
    "all_target_words = get_vocab(lines[lang_code])\n",
    "num_encoder_tokens = len(all_source_words)\n",
    "num_decoder_tokens = len(all_target_words)\n",
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens\n",
    "print(f'num_encoder_tokens: {num_encoder_tokens}')\n",
    "print(f'num_decoder_tokens: {num_decoder_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6300146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(sentences):\n",
    "    res = 0\n",
    "    for text in sentences:\n",
    "        res = max(res, len(text.split(' ')))\n",
    "    return res\n",
    "\n",
    "max_length_src = get_max_length(lines['eng'])\n",
    "max_length_tar = get_max_length(lines[lang_code])\n",
    "print(f'source sentences max length: {max_length_src}')\n",
    "print(f'target sentences max length: {max_length_tar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a483fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = sorted(list(all_source_words))\n",
    "target_words = sorted(list(all_target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = shuffle(lines)\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a13c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = lines['eng'], lines[lang_code]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86326958",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p input/train\n",
    "!mkdir -p input/test\n",
    "!mkdir -p input/vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(input_words, 'input/vocab/source.txt')\n",
    "save_vocab(target_words, 'input/vocab/target.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51badf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index, reverse_input_index = load_vocab('input/vocab/source.txt')\n",
    "target_token_index, reverse_target_index = load_vocab('input/vocab/target.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input/train/x.npy', X_train)\n",
    "np.save('input/train/y.npy', y_train)\n",
    "np.save('input/test/x.npy', X_test)\n",
    "np.save('input/test/y.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {\n",
    "    'max_length_source': max_length_src,\n",
    "    'max_length_target': max_length_tar\n",
    "}\n",
    "with open('input/vocab/meta.json', 'w') as f:\n",
    "    json.dump(meta, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b71e6d",
   "metadata": {},
   "source": [
    "## Test training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "03d96c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    epochs=30,\n",
    "    batch_size=4,\n",
    "    latent_dim=50,\n",
    "    verbose=1,\n",
    "    model_save_dir='./model',\n",
    "    train_dir='./input/train',\n",
    "    test_dir='./input/test',\n",
    "    vocab_dir='./input/vocab'\n",
    ")\n",
    "\n",
    "from train import train\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4023ad4d",
   "metadata": {},
   "source": [
    "## Upload training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# region = 'us-east-1'\n",
    "region = 'us-west-2'\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "session = sagemaker.Session(boto_session=boto_session)\n",
    "role = sagemaker.get_execution_role()\n",
    "session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f'lstm-translator-{lang_code}'\n",
    "\n",
    "train_input_path   = session.upload_data('input/train', key_prefix=prefix+'/train')\n",
    "test_input_path    = session.upload_data('input/test', key_prefix=prefix+'/test')\n",
    "vocab_input_path   = session.upload_data('input/vocab', key_prefix=prefix+'/vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c74984",
   "metadata": {},
   "source": [
    "## Launch SageMaker training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64519ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "local = False\n",
    "\n",
    "local_hps = {\n",
    "    'epochs': 1,\n",
    "    'batch-size': 1,\n",
    "    'latent-dim': 20\n",
    "}\n",
    "\n",
    "train_hps = {\n",
    "    'epochs': 30,\n",
    "    'batch-size': 64,\n",
    "    'latent-dim': 50\n",
    "}\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "    source_dir='src',\n",
    "    entry_point='train.py', \n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type='local' if local else 'ml.g4dn.2xlarge',\n",
    "    framework_version='2.8', \n",
    "    py_version='py39',\n",
    "    script_mode=True,\n",
    "    hyperparameters=local_hps if local else train_hps,\n",
    "    sagemaker_session=session,\n",
    "    metric_definitions=[\n",
    "        {'Name': 'train:loss', 'Regex': 'loss:\\s([\\d\\.]*?)\\s'},\n",
    "        {'Name': 'train:accuracy', 'Regex': 'acc:\\s([\\d\\.]*?)\\s'},\n",
    "        {'Name': 'val:loss', 'Regex': 'val_loss:\\s([\\d\\.]*?)\\s'},\n",
    "        {'Name': 'val:accuracy', 'Regex': 'val_acc:\\s([\\d\\.]*?)\\s'}\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({\n",
    "    'train': train_input_path, \n",
    "    'test': test_input_path,\n",
    "    'vocab': vocab_input_path\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc588819",
   "metadata": {},
   "source": [
    "## Download trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f39704",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://{session.default_bucket()}/{tf_estimator.jobs[0].name}/output/model.tar.gz ./model\n",
    "!cd model && tar -xvf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c2c67",
   "metadata": {},
   "source": [
    "## Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a235496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0439bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_weights = './model/weights-16.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b63450",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(latent_dim=50, vocab_dir='./input/vocab', weights_file=selected_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "41576177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tom sta ancora imparando'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('Tom is still learning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40cb475a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hanno detto perché'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('Did they say why?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad7133ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'io vado a scuola'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('I am going to school.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7829aa90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tom ha preso in prestito un libro da mary'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('Tom borrowed a book from Mary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51b32db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'io ho miei cani da mangiare'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('I feed my dog.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e5212",
   "metadata": {},
   "source": [
    "<font size=\"10\">🧐</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa3ea6",
   "metadata": {},
   "source": [
    "### Draw model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d081d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydot\n",
    "%pip install pydotplus\n",
    "!sudo yum -y install graphviz\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = plot_model(predictor.encoder_model, show_shapes=True)\n",
    "image.width = 350\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388583a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = plot_model(predictor.decoder_model, show_shapes=True)\n",
    "image.width = 1000\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
