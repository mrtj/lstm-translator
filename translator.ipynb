{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5e1165",
   "metadata": {},
   "source": [
    "# Word-level translator using LSTMs\n",
    "\n",
    "This notebook shows how to train with Amazon SageMaker an NLP model that translates from English to a series of languages. The list of languages is limited by the available training data.\n",
    "\n",
    "The model uses an LSTM based encoder / decoder architeecture, inspired by https://github.com/hlamba28/Word-Level-Eng-Mar-NMT.\n",
    "\n",
    "The training time depends on the dataset available. Typically an epoch is trained in 30 minutes for small datasets (for example, Hungarian) and 1.5 - 2 hours for big datasets (for example, Italian). With 15-20 epochs you can already get an acceptable result.\n",
    "\n",
    "The dataset was created from a curated list of anki flash cards (https://ankiweb.net/shared/decks/). Each translated sentences is annotated with CC license string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b1199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "sys.path.append('./src')\n",
    "\n",
    "from utils import preproc, load_vocab, save_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93294e70",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields(l):\n",
    "    l = l.strip()\n",
    "    elems = l.split(' - ')\n",
    "    source_name = elems[0]\n",
    "    elems = elems[1].split()\n",
    "    target_name = elems[0]\n",
    "    file_name = elems[1]\n",
    "    num_sentences = elems[2].strip('()')\n",
    "    base_name = file_name.split('.')[0]\n",
    "    source_code, target_code = base_name.split('-')\n",
    "    return source_name, source_code, target_name, target_code, file_name, num_sentences\n",
    "    \n",
    "with open('./dataset/language_list.txt') as f:\n",
    "    data = [extract_fields(l) for l in f]\n",
    "    cols = ['source_name', 'source_code', 'target_name', 'target_code', 'file_name', 'num_sentences']\n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2bf8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all possible language pairs\n",
    "\n",
    "dataset_path = Path('dataset')\n",
    "\n",
    "!mkdir -p {dataset_path}\n",
    "\n",
    "ip = get_ipython()\n",
    "for lang_code in df['source_code']:\n",
    "    ip.system(\n",
    "    f'''cd {dataset_path} && \\\n",
    "        curl -O http://www.manythings.org/anki/{lang_code}-eng.zip && \\\n",
    "        cd -\n",
    "    ''')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf11574",
   "metadata": {},
   "source": [
    "## Select target language code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb924d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_code = 'ita'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a935674c",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec88c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(dataset_path / f'{lang_code}-eng.zip') as archive:\n",
    "    with archive.open(f'{lang_code}.txt') as f:\n",
    "        lines = pd.read_table(f, names=['eng', lang_code, 'attrib'])\n",
    "lines = lines[['eng', lang_code]]\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41defbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['eng'] = lines['eng'].apply(preproc)\n",
    "lines[lang_code] = lines[lang_code].apply(preproc)\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "lines[lang_code] = lines[lang_code].apply(lambda x : 'START_ '+ x + ' _END')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90991a26",
   "metadata": {},
   "source": [
    "## Create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(sentences):\n",
    "    vocab = set()\n",
    "    for text in sentences:\n",
    "        vocab.update(text.split())\n",
    "    return vocab\n",
    "\n",
    "all_source_words = get_vocab(lines['eng'])\n",
    "all_target_words = get_vocab(lines[lang_code])\n",
    "num_encoder_tokens = len(all_source_words)\n",
    "num_decoder_tokens = len(all_target_words)\n",
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens\n",
    "print(f'num_encoder_tokens: {num_encoder_tokens}')\n",
    "print(f'num_decoder_tokens: {num_decoder_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0854747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(sentences):\n",
    "    res = 0\n",
    "    for text in sentences:\n",
    "        res = max(res, len(text.split(' ')))\n",
    "    return res\n",
    "\n",
    "max_length_src = get_max_length(lines['eng'])\n",
    "max_length_tar = get_max_length(lines[lang_code])\n",
    "print(f'source sentences max length: {max_length_src}')\n",
    "print(f'target sentences max length: {max_length_tar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3699051",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = sorted(list(all_source_words))\n",
    "target_words = sorted(list(all_target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6241f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = shuffle(lines)\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b98f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = lines['eng'], lines[lang_code]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p input/train\n",
    "!mkdir -p input/test\n",
    "!mkdir -p input/vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(input_words, 'input/vocab/source.txt')\n",
    "save_vocab(target_words, 'input/vocab/target.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5757e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index, reverse_input_index = load_vocab('input/vocab/source.txt')\n",
    "target_token_index, reverse_target_index = load_vocab('input/vocab/target.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ca226",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input/train/x.npy', X_train)\n",
    "np.save('input/train/y.npy', y_train)\n",
    "np.save('input/test/x.npy', X_test)\n",
    "np.save('input/test/y.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfefe254",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {\n",
    "    'max_length_source': max_length_src,\n",
    "    'max_length_target': max_length_tar\n",
    "}\n",
    "with open('input/vocab/meta.json', 'w') as f:\n",
    "    json.dump(meta, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d6b35",
   "metadata": {},
   "source": [
    "## Test training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f968de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a3d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    epochs=30,\n",
    "    batch_size=4,\n",
    "    latent_dim=50,\n",
    "    model_dir='./model',\n",
    "    train_dir='./input/train',\n",
    "    test_dir='./input/test',\n",
    "    vocab_dir='./input/vocab'\n",
    ")\n",
    "\n",
    "from src import train\n",
    "reload(train)\n",
    "from src.train import train\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea9126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_samples = len(X_train)\n",
    "train_samples // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29f8d5",
   "metadata": {},
   "source": [
    "## Upload training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ab45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# region = 'us-east-1'\n",
    "region = 'us-west-2'\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "session = sagemaker.Session(boto_session=boto_session)\n",
    "role = sagemaker.get_execution_role()\n",
    "session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8868620",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f'lstm-translator-{lang_code}'\n",
    "\n",
    "train_input_path   = session.upload_data('input/train', key_prefix=prefix+'/train')\n",
    "test_input_path    = session.upload_data('input/test', key_prefix=prefix+'/test')\n",
    "vocab_input_path   = session.upload_data('input/vocab', key_prefix=prefix+'/vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd7a434",
   "metadata": {},
   "source": [
    "## Launch SageMaker training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "local = False\n",
    "\n",
    "local_hps = {\n",
    "    'epochs': 1,\n",
    "    'batch-size': 1,\n",
    "    'latent-dim': 20\n",
    "}\n",
    "\n",
    "train_hps = {\n",
    "    'epochs': 30,\n",
    "    'batch-size': 64,\n",
    "    'latent-dim': 50\n",
    "}\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "    source_dir='src',\n",
    "    entry_point='train.py', \n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type='local' if local else 'ml.g4dn.2xlarge',\n",
    "    framework_version='2.8', \n",
    "    py_version='py39',\n",
    "    script_mode=True,\n",
    "    hyperparameters=local_hps if local else train_hps,\n",
    "    sagemaker_session=session,\n",
    "    metric_definitions=[\n",
    "        {'Name': 'train:loss', 'Regex': 'loss:\\s([\\d\\.]*?)\\s'},\n",
    "        {'Name': 'train:accuracy', 'Regex': 'acc:\\s([\\d\\.]*?)\\s'},\n",
    "        {'Name': 'val:loss', 'Regex': 'val_loss:\\s([\\d\\.]*?)\\s'},\n",
    "        {'Name': 'val:accuracy', 'Regex': 'val_acc:\\s([\\d\\.]*?)\\s'}\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({\n",
    "    'train': train_input_path, \n",
    "    'test': test_input_path,\n",
    "    'vocab': vocab_input_path\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be62465",
   "metadata": {},
   "source": [
    "## Download trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://{session.default_bucket()}/{tf_estimator.jobs[0].name}/output/model.tar.gz ./model\n",
    "!cd model && tar -xvf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34351eae",
   "metadata": {},
   "source": [
    "## Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ffe3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46591b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(latent_dim=50, vocab_dir='./input/vocab', weights_file='./model/weights-16.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2cea2050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tom sta ancora imparando'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('Tom is still learning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37c196e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hanno detto perché'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('Did they say why?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "860b4da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'io vado a scuola'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('I am going to school.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4bb6650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tom ha preso in prestito un libro da mary'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('Tom borrowed a book from Mary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6bf4b1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'io ho miei cani da mangiare'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('I feed my dog.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891f842",
   "metadata": {},
   "source": [
    "<font size=\"10\">🧐</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db712a6d",
   "metadata": {},
   "source": [
    "### Draw model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ea183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydot\n",
    "%pip install pydotplus\n",
    "!sudo yum -y install graphviz\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f13a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = plot_model(predictor.encoder_model, show_shapes=True)\n",
    "image.width = 350\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = plot_model(predictor.decoder_model, show_shapes=True)\n",
    "image.width = 1000\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
