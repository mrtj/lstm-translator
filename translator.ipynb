{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85637b71",
   "metadata": {},
   "source": [
    "# Word-level translator using LSTMs\n",
    "\n",
    "This notebook shows how to train with Amazon SageMaker an NLP model that translates from English to a series of languages. The list of languages is limited by the available training data.\n",
    "\n",
    "The model uses an LSTM based encoder / decoder architeecture, inspired by https://github.com/hlamba28/Word-Level-Eng-Mar-NMT.\n",
    "\n",
    "The training time depends on the dataset available. Typically an epoch is trained in 30 minutes for small datasets (for example, Hungarian) and 1.5 - 2 hours for big datasets (for example, Italian). With 15-20 epochs you can already get an acceptable result.\n",
    "\n",
    "The dataset was created from a curated list of anki flash cards (https://ankiweb.net/shared/decks/). Each translated sentences is annotated with CC license string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fcfa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "sys.path.append('./src')\n",
    "\n",
    "from utils import preproc, load_vocab, save_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b352b296",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "01147611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_name</th>\n",
       "      <th>source_code</th>\n",
       "      <th>target_name</th>\n",
       "      <th>target_code</th>\n",
       "      <th>file_name</th>\n",
       "      <th>num_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>afr</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>afr-eng.zip</td>\n",
       "      <td>904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albanian</td>\n",
       "      <td>sqi</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>sqi-eng.zip</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algerian Arabic</td>\n",
       "      <td>arq</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>arq-eng.zip</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>ara</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>ara-eng.zip</td>\n",
       "      <td>12156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Assamese</td>\n",
       "      <td>asm</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>asm-eng.zip</td>\n",
       "      <td>1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Ukrainian</td>\n",
       "      <td>ukr</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>ukr-eng.zip</td>\n",
       "      <td>155019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Urdu</td>\n",
       "      <td>urd</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>urd-eng.zip</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Uyghur</td>\n",
       "      <td>uig</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>uig-eng.zip</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>vie</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>vie-eng.zip</td>\n",
       "      <td>8050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Waray</td>\n",
       "      <td>war</td>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td>war-eng.zip</td>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source_name source_code target_name target_code    file_name  \\\n",
       "0         Afrikaans         afr     English         eng  afr-eng.zip   \n",
       "1          Albanian         sqi     English         eng  sqi-eng.zip   \n",
       "2   Algerian Arabic         arq     English         eng  arq-eng.zip   \n",
       "3            Arabic         ara     English         eng  ara-eng.zip   \n",
       "4          Assamese         asm     English         eng  asm-eng.zip   \n",
       "..              ...         ...         ...         ...          ...   \n",
       "77        Ukrainian         ukr     English         eng  ukr-eng.zip   \n",
       "78             Urdu         urd     English         eng  urd-eng.zip   \n",
       "79           Uyghur         uig     English         eng  uig-eng.zip   \n",
       "80       Vietnamese         vie     English         eng  vie-eng.zip   \n",
       "81            Waray         war     English         eng  war-eng.zip   \n",
       "\n",
       "   num_sentences  \n",
       "0            904  \n",
       "1            448  \n",
       "2            156  \n",
       "3          12156  \n",
       "4           1963  \n",
       "..           ...  \n",
       "77        155019  \n",
       "78          1146  \n",
       "79           281  \n",
       "80          8050  \n",
       "81          1147  \n",
       "\n",
       "[82 rows x 6 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_fields(l):\n",
    "    l = l.strip()\n",
    "    elems = l.split(' - ')\n",
    "    source_name = elems[0]\n",
    "    elems = elems[1].split()\n",
    "    target_name = elems[0]\n",
    "    file_name = elems[1]\n",
    "    num_sentences = elems[2].strip('()')\n",
    "    base_name = file_name.split('.')[0]\n",
    "    source_code, target_code = base_name.split('-')\n",
    "    return source_name, source_code, target_name, target_code, file_name, num_sentences\n",
    "    \n",
    "with open('./dataset/language_list.txt') as f:\n",
    "    data = [extract_fields(l) for l in f]\n",
    "    cols = ['source_name', 'source_code', 'target_name', 'target_code', 'file_name', 'num_sentences']\n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all possible language pairs\n",
    "\n",
    "dataset_path = Path('dataset')\n",
    "\n",
    "!mkdir -p {dataset_path}\n",
    "\n",
    "ip = get_ipython()\n",
    "for lang_code in df['source_code']:\n",
    "    ip.system(\n",
    "    f'''cd {dataset_path} && \\\n",
    "        curl -O http://www.manythings.org/anki/{lang_code}-eng.zip && \\\n",
    "        cd -\n",
    "    ''')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ae789",
   "metadata": {},
   "source": [
    "## Select target language code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4cf08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_code = 'ita'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226a051",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(dataset_path / f'{lang_code}-eng.zip') as archive:\n",
    "    with archive.open(f'{lang_code}.txt') as f:\n",
    "        lines = pd.read_table(f, names=['eng', lang_code, 'attrib'])\n",
    "lines = lines[['eng', lang_code]]\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e2b8493c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>ita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187311</th>\n",
       "      <td>its toms first time here</td>\n",
       "      <td>START_ start è la prima volta di tom qui end _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134927</th>\n",
       "      <td>he was wearing a tuxedo</td>\n",
       "      <td>START_ start lui stava indossando uno smoking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68322</th>\n",
       "      <td>try to describe it</td>\n",
       "      <td>START_ start cercate di descriverla end _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219433</th>\n",
       "      <td>my car was stolen last night</td>\n",
       "      <td>START_ start mi è stata rubata la macchina la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33673</th>\n",
       "      <td>youre so lucky</td>\n",
       "      <td>START_ start lei è così fortunato end _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132020</th>\n",
       "      <td>youre not fast enough</td>\n",
       "      <td>START_ start lei non è abbastanza veloce end _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120877</th>\n",
       "      <td>i know tom is a smoker</td>\n",
       "      <td>START_ start so che tom è un fumatore end _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151223</th>\n",
       "      <td>hunger is the best sauce</td>\n",
       "      <td>START_ start la fame è il miglior condimento e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311628</th>\n",
       "      <td>my wife is preparing dinner right now</td>\n",
       "      <td>START_ start mia moglie sta preparando la cena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247091</th>\n",
       "      <td>its healthy to breathe deeply</td>\n",
       "      <td>START_ start è salutare respirare profondament...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          eng  \\\n",
       "187311               its toms first time here   \n",
       "134927                he was wearing a tuxedo   \n",
       "68322                      try to describe it   \n",
       "219433           my car was stolen last night   \n",
       "33673                          youre so lucky   \n",
       "132020                  youre not fast enough   \n",
       "120877                 i know tom is a smoker   \n",
       "151223               hunger is the best sauce   \n",
       "311628  my wife is preparing dinner right now   \n",
       "247091          its healthy to breathe deeply   \n",
       "\n",
       "                                                      ita  \n",
       "187311  START_ start è la prima volta di tom qui end _END  \n",
       "134927  START_ start lui stava indossando uno smoking ...  \n",
       "68322        START_ start cercate di descriverla end _END  \n",
       "219433  START_ start mi è stata rubata la macchina la ...  \n",
       "33673          START_ start lei è così fortunato end _END  \n",
       "132020  START_ start lei non è abbastanza veloce end _END  \n",
       "120877     START_ start so che tom è un fumatore end _END  \n",
       "151223  START_ start la fame è il miglior condimento e...  \n",
       "311628  START_ start mia moglie sta preparando la cena...  \n",
       "247091  START_ start è salutare respirare profondament...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines['eng'] = lines['eng'].apply(preproc)\n",
    "lines[lang_code] = lines[lang_code].apply(preproc)\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "lines[lang_code] = lines[lang_code].apply(lambda x : 'START_ '+ x + ' _END')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45491895",
   "metadata": {},
   "source": [
    "## Create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(sentences):\n",
    "    vocab = set()\n",
    "    for text in sentences:\n",
    "        vocab.update(text.split())\n",
    "    return vocab\n",
    "\n",
    "all_source_words = get_vocab(lines['eng'])\n",
    "all_target_words = get_vocab(lines[lang_code])\n",
    "num_encoder_tokens = len(all_source_words)\n",
    "num_decoder_tokens = len(all_target_words)\n",
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens\n",
    "print(f'num_encoder_tokens: {num_encoder_tokens}')\n",
    "print(f'num_decoder_tokens: {num_decoder_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fb300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(sentences):\n",
    "    res = 0\n",
    "    for text in sentences:\n",
    "        res = max(res, len(text.split(' ')))\n",
    "    return res\n",
    "\n",
    "max_length_src = get_max_length(lines['eng'])\n",
    "max_length_tar = get_max_length(lines[lang_code])\n",
    "print(f'source sentences max length: {max_length_src}')\n",
    "print(f'target sentences max length: {max_length_tar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = sorted(list(all_source_words))\n",
    "target_words = sorted(list(all_target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e349c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = shuffle(lines)\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = lines['eng'], lines[lang_code]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b082e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p input/train\n",
    "!mkdir -p input/test\n",
    "!mkdir -p input/vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(input_words, 'input/vocab/source.txt')\n",
    "save_vocab(target_words, 'input/vocab/target.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226528de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index, reverse_input_index = load_vocab('input/vocab/source.txt')\n",
    "target_token_index, reverse_target_index = load_vocab('input/vocab/target.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be11487",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input/train/x.npy', X_train)\n",
    "np.save('input/train/y.npy', y_train)\n",
    "np.save('input/test/x.npy', X_test)\n",
    "np.save('input/test/y.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ae9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {\n",
    "    'max_length_source': max_length_src,\n",
    "    'max_length_target': max_length_tar\n",
    "}\n",
    "with open('input/vocab/meta.json', 'w') as f:\n",
    "    json.dump(meta, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d9242",
   "metadata": {},
   "source": [
    "## Test training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1404202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d3499",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    epochs=30,\n",
    "    batch_size=4,\n",
    "    latent_dim=50,\n",
    "    verbose=1,\n",
    "    model_save_dir='./model',\n",
    "    train_dir='./input/train',\n",
    "    test_dir='./input/test',\n",
    "    vocab_dir='./input/vocab'\n",
    ")\n",
    "\n",
    "from train import train\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e2fb3",
   "metadata": {},
   "source": [
    "## Upload training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422dbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# region = 'us-east-1'\n",
    "region = 'us-west-2'\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "session = sagemaker.Session(boto_session=boto_session)\n",
    "role = sagemaker.get_execution_role()\n",
    "session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f'lstm-translator-{lang_code}'\n",
    "\n",
    "train_input_path   = session.upload_data('input/train', key_prefix=prefix+'/train')\n",
    "test_input_path    = session.upload_data('input/test', key_prefix=prefix+'/test')\n",
    "vocab_input_path   = session.upload_data('input/vocab', key_prefix=prefix+'/vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395fd100",
   "metadata": {},
   "source": [
    "## Launch SageMaker training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506596ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "local = False\n",
    "\n",
    "local_hps = {\n",
    "    'epochs': 1,\n",
    "    'batch-size': 1,\n",
    "    'latent-dim': 20\n",
    "}\n",
    "\n",
    "train_hps = {\n",
    "    'epochs': 30,\n",
    "    'batch-size': 64,\n",
    "    'latent-dim': 50\n",
    "}\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "    source_dir='src',\n",
    "    entry_point='train.py', \n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type='local' if local else 'ml.g4dn.2xlarge',\n",
    "    framework_version='2.8', \n",
    "    py_version='py39',\n",
    "    script_mode=True,\n",
    "    hyperparameters=local_hps if local else train_hps,\n",
    "    sagemaker_session=session,\n",
    "    metric_definitions=[\n",
    "        {'Name': 'train:loss', 'Regex': 'loss:\\s([\\d\\.]*?)\\s'},\n",
    "        {'Name': 'train:accuracy', 'Regex': 'acc:\\s([\\d\\.]*?)\\s'},\n",
    "        {'Name': 'val:loss', 'Regex': 'val_loss:\\s([\\d\\.]*?)\\s'},\n",
    "        {'Name': 'val:accuracy', 'Regex': 'val_acc:\\s([\\d\\.]*?)\\s'}\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3feb637",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({\n",
    "    'train': train_input_path, \n",
    "    'test': test_input_path,\n",
    "    'vocab': vocab_input_path\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2670d8",
   "metadata": {},
   "source": [
    "## Download trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2663b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://{session.default_bucket()}/{tf_estimator.jobs[0].name}/output/model.tar.gz ./model\n",
    "!cd model && tar -xvf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79169db6",
   "metadata": {},
   "source": [
    "## Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_weights = './model/weights-16.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(latent_dim=50, vocab_dir='./input/vocab', weights_file=selected_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1788eed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tom sta ancora imparando'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('Tom is still learning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6558057f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hanno detto perché'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('Did they say why?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "491d0258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'io vado a scuola'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('I am going to school.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c887fd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tom ha preso in prestito un libro da mary'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('Tom borrowed a book from Mary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c3c51ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'io ho miei cani da mangiare'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('I feed my dog.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9f68d",
   "metadata": {},
   "source": [
    "<font size=\"10\">🧐</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d60744",
   "metadata": {},
   "source": [
    "### Draw model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d147257",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydot\n",
    "%pip install pydotplus\n",
    "!sudo yum -y install graphviz\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = plot_model(predictor.encoder_model, show_shapes=True)\n",
    "image.width = 350\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28849776",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = plot_model(predictor.decoder_model, show_shapes=True)\n",
    "image.width = 1000\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
